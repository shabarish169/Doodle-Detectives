{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:07.861362Z","iopub.status.busy":"2023-09-16T04:38:07.860778Z","iopub.status.idle":"2023-09-16T04:38:14.931268Z","shell.execute_reply":"2023-09-16T04:38:14.930117Z","shell.execute_reply.started":"2023-09-16T04:38:07.861321Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import numpy as np, pandas as pd, matplotlib.pyplot as plt\n","import torch, torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from sklearn import preprocessing\n","from sklearn.model_selection import StratifiedKFold, KFold\n","import torchvision.models, torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","device = torch.device('cuda:0')\n","\"\"\"\n","We did not submit this successfully. Our submission.csv had duplicate ids due to an error.\n","We used ResNet18 to make an ensemble using cross validation (though we did not actually do validation due to time constraints).\n","The code at the end was made in an attempt to fix our submission file at the last minute.\n","This used a set of pretrained weights made when our code failed in a save and commit as well as the weights from our final working submission.\n","\"\"\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:14.934117Z","iopub.status.busy":"2023-09-16T04:38:14.933420Z","iopub.status.idle":"2023-09-16T04:38:14.939852Z","shell.execute_reply":"2023-09-16T04:38:14.938783Z","shell.execute_reply.started":"2023-09-16T04:38:14.934076Z"},"trusted":true},"outputs":[],"source":["# import gc\n","# gc.collect()\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:14.942366Z","iopub.status.busy":"2023-09-16T04:38:14.941602Z","iopub.status.idle":"2023-09-16T04:38:14.955625Z","shell.execute_reply":"2023-09-16T04:38:14.954557Z","shell.execute_reply.started":"2023-09-16T04:38:14.942324Z"},"trusted":true},"outputs":[],"source":["from PIL import Image, ImageDraw\n","import numpy as np\n","import json\n","\n","def vector_to_numpy(drawing, side=256):\n","    image = vector_to_image(drawing, side)\n","    image_array = np.array(image)\n","    return image_array\n","\n","def vector_to_image(drawing, side=256):\n","    drawing = json.loads(drawing)\n","    min_x, min_y, max_x, max_y = calculate_bounding_box(drawing)\n","\n","    # Calculate the offset to center the drawing within the canvas\n","    offset_x = (side - (max_x - min_x + 1)) // 2\n","    offset_y = (side - (max_y - min_y + 1)) // 2\n","\n","    image = Image.new('L', (side, side), color='white')  # Create a white canvas\n","    draw = ImageDraw.Draw(image)\n","\n","    for x, y in drawing:\n","        xy = [(x0 - min_x + offset_x, y0 - min_y + offset_y) for x0, y0 in zip(x, y)]\n","        draw.line(xy, fill='black', width=1)\n","\n","    return image\n","\n","def calculate_bounding_box(drawing):\n","    all_x = [x for x, _ in drawing]\n","    all_y = [y for _, y in drawing]\n","\n","    min_x = min(min(x) for x in all_x)\n","    min_y = min(min(y) for y in all_y)\n","    max_x = max(max(x) for x in all_x)\n","    max_y = max(max(y) for y in all_y)\n","\n","    return min_x, min_y, max_x, max_y"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:14.959292Z","iopub.status.busy":"2023-09-16T04:38:14.958921Z","iopub.status.idle":"2023-09-16T04:38:20.665306Z","shell.execute_reply":"2023-09-16T04:38:20.664182Z","shell.execute_reply.started":"2023-09-16T04:38:14.959257Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/train.csv') #, dtype={'drawing': np.array})\n","class_list = df['word'].unique()\n","classes = {word: index for index, word in enumerate(class_list)}\n","def prediction_to_words(prediction):\n","    return ' '.join((class_list[p] for p in prediction))\n","# df = df[df['recognized']==True].reset_index().drop('index', axis=1)\n","df = df.reset_index().drop('index', axis=1)\n","# df = df[df['word'].isin(class_list[:10])].reset_index().drop('index', axis=1)\n","# df['drawing'] = df['drawing'].map(vector_to_numpy)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:20.667258Z","iopub.status.busy":"2023-09-16T04:38:20.666886Z","iopub.status.idle":"2023-09-16T04:38:20.672031Z","shell.execute_reply":"2023-09-16T04:38:20.671043Z","shell.execute_reply.started":"2023-09-16T04:38:20.667215Z"},"trusted":true},"outputs":[],"source":["# print(classes, class_list, sep='\\n') /kaggle/input/doodle-detectives-aiclubiitm/sample_submission.csv"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:20.674182Z","iopub.status.busy":"2023-09-16T04:38:20.673550Z","iopub.status.idle":"2023-09-16T04:38:25.961376Z","shell.execute_reply":"2023-09-16T04:38:25.960317Z","shell.execute_reply.started":"2023-09-16T04:38:20.674149Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","#      transforms.Resize((28, 28)),\n","     transforms.Lambda(lambda x: x.repeat(1,3, 1, 1)),\n","    #  transforms.Lambda(lambda x: print(x.shape)),\n","    #  transforms.Grayscale(num_output_channels=3),\n","    #  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","     ])\n","vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n","# inception_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n","inception_v3 = torchvision.models.inception_v3(weights=torchvision.models.Inception_V3_Weights.DEFAULT)\n","inception_v3.aux_logits = False\n","resnet_18 = torchvision.models.resnet18()#weights=torchvision.models.ResNet18_Weights.DEFAULT)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:25.963513Z","iopub.status.busy":"2023-09-16T04:38:25.963111Z","iopub.status.idle":"2023-09-16T04:38:25.982587Z","shell.execute_reply":"2023-09-16T04:38:25.981487Z","shell.execute_reply.started":"2023-09-16T04:38:25.963477Z"},"trusted":true},"outputs":[],"source":["class BasicCNN(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = nn.Sequential(\n","            nn.MaxPool2d(8),\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n","            # 1 input image. If we had an RGB image, it would be Conv2d(3, 32, 3, padding=1)\n","            # 32 output images, i.e, 32 kernels and 32 output images are produced\n","            nn.ReLU(),\n","            # The activation function\n","            nn.MaxPool2d(2),\n","            # Pooling with 2 x 2 blocks\n","            nn.Conv2d(32, 64, 3, padding=1),\n","            # Now we have those 32 images and we make 64 from them\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","            # Pooling again\n","        )\n","        self.fully_connected = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear((28*28*64)//16, 512),\n","            # The image shape was initially 28 x 28, by pooling we've made it 7 x 7, so we divide by 16\n","            # We multiply by 64 because the model has learnt 64 features.\n","            nn.Linear(512, 101),\n","#             nn.Linear(128, 101)\n","            # We have 10 output neurons (1 for each class)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transforms.Compose([transforms.Resize((28, 28))])(inputs)\n","        x = self.convolutions(inputs.to(device))\n","        # Functions in convolution layers are run\n","        x = self.fully_connected(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='SUBMISSION.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)\n","\n","# le = preprocessing.LabelEncoder()\n","class MyDataset():\n","    def __init__(self, data, targets=None, ids=None, train=True, size=224):\n","        self.data = data\n","        self.train = train\n","        self.size = size\n","        if train: \n","            self.targets = targets.map(lambda target: classes[target])\n","        if ids is not None: self.ids = ids\n","    def __len__(self):\n","        return len(self.data)\n","    def __getitem__(self, i):\n","        img = 1 - vector_to_numpy(self.data.loc[i], side=self.size)//250\n","        if self.train: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.targets[i], dtype=torch.int64)\n","        if self.ids is not None: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.ids[i], dtype=torch.int64)\n","        return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape)))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:25.985012Z","iopub.status.busy":"2023-09-16T04:38:25.984318Z","iopub.status.idle":"2023-09-16T04:38:26.008963Z","shell.execute_reply":"2023-09-16T04:38:26.007707Z","shell.execute_reply.started":"2023-09-16T04:38:25.984978Z"},"trusted":true},"outputs":[],"source":["i = 0\n","vgg_length = len(list(vgg16.features.parameters()))\n","for param in vgg16.features.parameters():\n","    if i < vgg_length - 4: param.requires_grad = False\n","i = 0\n","inc_length = len(list(inception_v3.parameters()))\n","for param in inception_v3.parameters():\n","    if i < inc_length - 3: param.requires_grad = False\n","for param in resnet_18.parameters():\n","    param.requires_grad = True\n","resnet_18.fc = nn.Linear(512, 101)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:26.011492Z","iopub.status.busy":"2023-09-16T04:38:26.010760Z","iopub.status.idle":"2023-09-16T04:38:26.024426Z","shell.execute_reply":"2023-09-16T04:38:26.023429Z","shell.execute_reply.started":"2023-09-16T04:38:26.011455Z"},"trusted":true},"outputs":[],"source":["class VGG16(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = vgg16\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","        inputs = transform(inputs).to(device=device)\n","        x = self.convolutions(inputs)\n","        x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:26.030141Z","iopub.status.busy":"2023-09-16T04:38:26.029867Z","iopub.status.idle":"2023-09-16T04:38:26.043485Z","shell.execute_reply":"2023-09-16T04:38:26.042562Z","shell.execute_reply.started":"2023-09-16T04:38:26.030117Z"},"trusted":true},"outputs":[],"source":["class Inception(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = inception_v3\n","\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","#             nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transform(inputs).to(device=device)\n","        inputs = transforms.Compose([transforms.Resize((299, 299)),\n","                                    transforms.Lambda(lambda x: x.repeat(1,3, 1, 1))])(inputs)\n","        x = self.convolutions(inputs.to(device))\n","        x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:26.045595Z","iopub.status.busy":"2023-09-16T04:38:26.045240Z","iopub.status.idle":"2023-09-16T04:38:26.062323Z","shell.execute_reply":"2023-09-16T04:38:26.061279Z","shell.execute_reply.started":"2023-09-16T04:38:26.045562Z"},"trusted":true},"outputs":[],"source":["class ResNet18(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = resnet_18\n","\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs, device=torch.device('cuda:0')):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transform(inputs).to(device=device)\n","        inputs = transforms.Compose([\n","#                                     transforms.Resize((299, 299)),\n","                                    transforms.Lambda(lambda x: x.repeat(1,3, 1, 1))])(inputs)\n","        inputs = inputs.to(device)\n","#         print(inputs.get_device())\n","        x = self.convolutions(inputs)\n","#         x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions\n","        if out: total_predictions =[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            \n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                if (i%50 == 0 and i != 0):\n","#                     df = pd.DataFrame({'key_id': ids, 'word': total_predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                    # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                    total_predictions = pd.concat(total_predictions)\n","                    total_predictions.to_csv(f'{out_small}_{i//50}.csv', index=False)\n","                    total_predictions = []\n","                else:\n","                    total_predictions.append(pd.DataFrame({'key_id': ids, 'word': predictions}))\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            if total_predictions: \n","                total_predictions = pd.concat(total_predictions)\n","                total_predictions.to_csv(f'{out_small}_{i}.csv', index=False)\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:26.065303Z","iopub.status.busy":"2023-09-16T04:38:26.064588Z","iopub.status.idle":"2023-09-16T04:38:26.079054Z","shell.execute_reply":"2023-09-16T04:38:26.077980Z","shell.execute_reply.started":"2023-09-16T04:38:26.065270Z"},"trusted":true},"outputs":[],"source":["# Defining one epoch of training\n","def train(model, train_loader, optimizer, loss, ps=50):\n","    # We train the appropriate model with the input data and the appropriate optimizer\n","    # ps is how often we print the accuracy\n","    train_iter = iter(train_loader)\n","    model.train()\n","    # Puts model in train mode\n","    for i, (data, targets) in enumerate(train_iter):\n","        # i is iteration, data = 1 mini batch of images, targets = 1 mini batch target values\n","        # This repeats for all mini batches \n","        # print(targets)\n","        targets = targets.to(device)\n","        outputs = model.forward(data) # Forward pass\n","        loss_val = loss(outputs, targets) # Loss computation\n","        # print(targets)\n","        optimizer.zero_grad()  # Ensures gradients stored in optimizer are reset before each backward pass\n","        loss_val.backward() # Backward pass\n","        optimizer.step() # Backward pass\n","\n","        if ps and i % ps == 0:\n","            model.eval()\n","            # Puts model in evaluation mode, so we \n","            with torch.no_grad():\n","                print(f\"Batch {i}: Loss is {loss_val}\", end='; ')\n","                predicted = outputs.max(1)[1]\n","                correct = (predicted == targets).sum().item()\n","                accuracy = correct/len(targets)\n","                print(f\"Train accuracy is {accuracy*100:.3f}%\")\n","def accuracy(model, test):\n","    # Evaluate a model given a test loader\n","    model.eval()\n","    with torch.no_grad():\n","        count = 0\n","        correct = 0\n","        for data, targets in iter(test):\n","            targets = targets.to(device)\n","            outputs = model.forward(data)\n","#             predicted = outputs.max(1)[1] # Maximum output is predicted class\n","            predictions = torch.topk(outputs, 3, dim=1)[1]\n","            pred_1 = predictions[:, 0]\n","            pred_2 = predictions[:, 1]\n","            pred_3 = predictions[:, 2]\n","            count += len(targets) # Total length of datasetS\n","            correct += (pred_1 == targets).sum().item() + (pred_2 == targets).sum().item() + (pred_3 == targets).sum().item()\n","            # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n","        # print((predicted == targets).sum().item())\n","        accuracy = correct/count\n","        return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.282177Z","iopub.status.idle":"2023-09-15T16:13:23.282881Z","shell.execute_reply":"2023-09-15T16:13:23.282670Z","shell.execute_reply.started":"2023-09-15T16:13:23.282648Z"},"trusted":true},"outputs":[],"source":["basic_cnn = BasicCNN().to(device)\n","cnn_optimizer = torch.optim.Adam(basic_cnn.parameters(), lr=2e-3)\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.284224Z","iopub.status.idle":"2023-09-15T16:13:23.284952Z","shell.execute_reply":"2023-09-15T16:13:23.284744Z","shell.execute_reply.started":"2023-09-15T16:13:23.284722Z"},"trusted":true},"outputs":[],"source":["train_loaders = []\n","for train_index, val_index in KFold(n_splits=5).split(df['drawing'], df['word']):\n","    df_train = df.loc[train_index].reset_index().drop('index', axis=1)\n","    df_val = df.loc[val_index].reset_index().drop('index', axis=1)\n","    train_dataset = MyDataset(df_train['drawing'], df_train['word'])\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","    train_loaders.append(train_loader)\n","train_dataset = MyDataset(df_train['drawing'], df_train['word'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.286274Z","iopub.status.idle":"2023-09-15T16:13:23.288953Z","shell.execute_reply":"2023-09-15T16:13:23.288719Z","shell.execute_reply.started":"2023-09-15T16:13:23.288691Z"},"trusted":true},"outputs":[],"source":["# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.290351Z","iopub.status.idle":"2023-09-15T16:13:23.291107Z","shell.execute_reply":"2023-09-15T16:13:23.290867Z","shell.execute_reply.started":"2023-09-15T16:13:23.290845Z"},"trusted":true},"outputs":[],"source":["# print(next(iter(train_loader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.292407Z","iopub.status.idle":"2023-09-15T16:13:23.293146Z","shell.execute_reply":"2023-09-15T16:13:23.292905Z","shell.execute_reply.started":"2023-09-15T16:13:23.292881Z"},"trusted":true},"outputs":[],"source":["# train_loader.targets.loc[0]\n","df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.294440Z","iopub.status.idle":"2023-09-15T16:13:23.295203Z","shell.execute_reply":"2023-09-15T16:13:23.294962Z","shell.execute_reply.started":"2023-09-15T16:13:23.294939Z"},"trusted":true},"outputs":[],"source":["# train(basic_cnn, train_loader, cnn_optimizer, loss_fn)\n","# torch.save(basic_cnn.state_dict(), 'first_model.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.296545Z","iopub.status.idle":"2023-09-15T16:13:23.297320Z","shell.execute_reply":"2023-09-15T16:13:23.297113Z","shell.execute_reply.started":"2023-09-15T16:13:23.297090Z"},"trusted":true},"outputs":[],"source":["# vgg16_model = VGG16()\n","# vgg16_model.to(device)\n","# vgg16_optim = torch.optim.Adam(vgg16_model.parameters(), lr=2e-3)\n","# vgg_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.298657Z","iopub.status.idle":"2023-09-15T16:13:23.299426Z","shell.execute_reply":"2023-09-15T16:13:23.299214Z","shell.execute_reply.started":"2023-09-15T16:13:23.299191Z"},"trusted":true},"outputs":[],"source":["# inception_v3_model = Inception()\n","# inception_v3_model.to(device)\n","# inc_optim = torch.optim.Adam(inception_v3_model.parameters(), lr=2e-3)\n","# inc_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.300873Z","iopub.status.idle":"2023-09-15T16:13:23.301653Z","shell.execute_reply":"2023-09-15T16:13:23.301416Z","shell.execute_reply.started":"2023-09-15T16:13:23.301392Z"},"trusted":true},"outputs":[],"source":["# # train_dataset = MyDataset(df_train['drawing'], df_train['word'], size=299)\n","# # train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","# train(inception_v3_model, train_loader, inc_optim, inc_loss)\n","# torch.save(inception_v3_model.state_dict(), 'inception_v3_model.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.303152Z","iopub.status.idle":"2023-09-15T16:13:23.303947Z","shell.execute_reply":"2023-09-15T16:13:23.303706Z","shell.execute_reply.started":"2023-09-15T16:13:23.303682Z"},"trusted":true},"outputs":[],"source":["res_model = ResNet18()\n","res_model.to(device)\n","res_optim = torch.optim.Adam(res_model.parameters(), lr=2e-4)\n","res_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.305420Z","iopub.status.idle":"2023-09-15T16:13:23.306224Z","shell.execute_reply":"2023-09-15T16:13:23.305973Z","shell.execute_reply.started":"2023-09-15T16:13:23.305949Z"},"trusted":true},"outputs":[],"source":["epochs = 4\n","res_models = []\n","for j, train_loader in enumerate(train_loaders):\n","    res_models.append(ResNet18())\n","    res_models[-1].to(device)\n","    res_models[-1].load_state_dict(torch.load('/kaggle/input/doodle-detectives-pretrained-model-weights/ResNet18_working_resized_1val.pt'))\n","    for i in range(epochs):\n","        print(f'Epoch {i+1}/20\\n')\n","        train(res_models[j], train_loaders[j], res_optim, res_loss)\n","        torch.save(res_models[j].state_dict(), f'ResNet18_{j}.pt')\n","    torch.save(res_models[j].state_dict(), f'Resnet18_{j}_final.pt')\n","    res_models[j].to(torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.307640Z","iopub.status.idle":"2023-09-15T16:13:23.308477Z","shell.execute_reply":"2023-09-15T16:13:23.308238Z","shell.execute_reply.started":"2023-09-15T16:13:23.308212Z"},"trusted":true},"outputs":[],"source":["val_set = MyDataset(df_val['drawing'], df_val['word'])\n","val_loader = DataLoader(val_set, batch_size=128)\n","# print(accuracy(basic_cnn, val_loader))/"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:26.082309Z","iopub.status.busy":"2023-09-16T04:38:26.080205Z","iopub.status.idle":"2023-09-16T04:38:28.050295Z","shell.execute_reply":"2023-09-16T04:38:28.049220Z","shell.execute_reply.started":"2023-09-16T04:38:26.082271Z"},"trusted":true},"outputs":[],"source":["dft = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/test.csv')#, dtype={'drawing': np.array})\n","# dft['drawing'] = dft['drawing'].map(vector_to_numpy)\n","# dft = dft[dft['word'].isin(class_list[:10])].reset_index().drop('index', axis=1)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:28.052187Z","iopub.status.busy":"2023-09-16T04:38:28.051820Z","iopub.status.idle":"2023-09-16T04:38:28.058973Z","shell.execute_reply":"2023-09-16T04:38:28.057701Z","shell.execute_reply.started":"2023-09-16T04:38:28.052152Z"},"trusted":true},"outputs":[],"source":["test_dataset = MyDataset(dft['drawing'], train=False, ids=dft['key_id'])\n","test_loader = DataLoader(test_dataset, batch_size=128, drop_last=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:28.061441Z","iopub.status.busy":"2023-09-16T04:38:28.060391Z","iopub.status.idle":"2023-09-16T04:38:28.070611Z","shell.execute_reply":"2023-09-16T04:38:28.069540Z","shell.execute_reply.started":"2023-09-16T04:38:28.061407Z"},"trusted":true},"outputs":[],"source":["# print(accuracy(vgg16_model, val_loader))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:28.072555Z","iopub.status.busy":"2023-09-16T04:38:28.071920Z","iopub.status.idle":"2023-09-16T04:38:28.083465Z","shell.execute_reply":"2023-09-16T04:38:28.082456Z","shell.execute_reply.started":"2023-09-16T04:38:28.072521Z"},"trusted":true},"outputs":[],"source":["# vgg16_model.predict(test_loader, i_0=406)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:28.087276Z","iopub.status.busy":"2023-09-16T04:38:28.084587Z","iopub.status.idle":"2023-09-16T04:38:28.093869Z","shell.execute_reply":"2023-09-16T04:38:28.092952Z","shell.execute_reply.started":"2023-09-16T04:38:28.087242Z"},"trusted":true},"outputs":[],"source":["# basic_cnn_2 = BasicCNN()\n","# basic_cnn_2.load_state_dict(torch.load('first_model.pt'))\n","# # basic_cnn_2.predict(test_loader)\n","# basic_cnn_3 = BasicCNN()\n","# # train(basic_cnn_3, train_loader, cnn_optimizer, loss_fn, ps=2)\n","\n","# with torch.no_grad():\n","#     print(basic_cnn_2.forward(next(iter(test_loader))[0]))\n","# # basic_cnn.eval()\n","# with torch.no_grad():\n","#     count = 0\n","#     correct = 0\n","#     for data, targets in iter(dataloader):\n","#         outputs = model.forward(data)\n","#         predicted = outputs.max(1)[1] # Maximum output is predicted class\n","#         count += len(targets) # Total length of datasetS\n","#         correct += (predicted == targets).sum().item()\n","#         # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n","#     print((predicted == targets).sum().item())\n","#     accuracy = correct/count\n","#     return accuracy\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:28.098112Z","iopub.status.busy":"2023-09-16T04:38:28.097631Z","iopub.status.idle":"2023-09-16T04:38:28.625023Z","shell.execute_reply":"2023-09-16T04:38:28.623599Z","shell.execute_reply.started":"2023-09-16T04:38:28.098087Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'res_model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(accuracy(basic_cnn, val_loader))\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(accuracy(inception_v3_model, val_loader))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy(\u001b[43mres_model\u001b[49m, val_loader))\n","\u001b[0;31mNameError\u001b[0m: name 'res_model' is not defined"]}],"source":["# print(accuracy(basic_cnn, val_loader))\n","# print(accuracy(inception_v3_model, val_loader))\n","print(accuracy(res_model, val_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T16:13:23.323034Z","iopub.status.idle":"2023-09-15T16:13:23.323796Z","shell.execute_reply":"2023-09-15T16:13:23.323576Z","shell.execute_reply.started":"2023-09-15T16:13:23.323552Z"},"trusted":true},"outputs":[],"source":["# basic_cnn.predict(test_loader)\n","# inception_v3_model.predict(test_loader)\n","# res_model.predict(test_loader)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:38:46.136574Z","iopub.status.busy":"2023-09-16T04:38:46.136003Z","iopub.status.idle":"2023-09-16T04:38:47.577073Z","shell.execute_reply":"2023-09-16T04:38:47.576011Z","shell.execute_reply.started":"2023-09-16T04:38:46.136531Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0\n","0\n","0\n"]}],"source":["res_model = ResNet18()\n","res_model.to(('cuda:0'))\n","device2 = torch.device('cuda:0')\n","res_model.load_state_dict(torch.load('/kaggle/input/doodle-detectives-pretrained-model-weights/ResNet18_working_resized_1val.pt'))\n","res_models = []\n","res_models.append(res_model)\n","for i in range(4):\n","    res_models.append(ResNet18())\n","    if i < 2: res_models[i+1].to(torch.device('cuda:0'))\n","    else: res_models[i+1].to(device2)\n","    res_models[i+1].load_state_dict(torch.load(f'/kaggle/input/resnet-doodle-detectives-more/Resnet18_{i}_final.pt'))\n","#     model.eval()\n","    print(next(res_models[0].parameters()).get_device())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T05:11:44.296109Z","iopub.status.busy":"2023-09-16T05:11:44.294961Z"},"trusted":true},"outputs":[],"source":["# res_model.predict(test_loader, out='SUBMISSION_RESNET_18_STAY_THIS_TIME.csv')\n","out = 'submission.csv'\n","out_small = 'sub'\n","if not out: total_predictions = []\n","if out: total_predictions =[]\n","for i, (data, ids) in enumerate(iter(test_loader)):\n","#     if i_0 > i: continue\n","    pred_list = []\n","    for j in range(5):\n","        with torch.no_grad():\n","            model = res_models[j]\n","            if j < 2:pred_list.append(model.forward(data, device).reshape(1, len(data), 101))\n","            else:pred_list.append(model.forward(data, device=device2).reshape(1, len(data), 101))\n","    pred_list = torch.concatenate(pred_list)\n","    pred_list = pred_list.sum(0)\n","#     print(pred_list)\n","    predictions = torch.topk(pred_list, 3, dim=1)[1]\n","    predictions = (prediction_to_words(p) for p in predictions)\n","    if out:\n","        if (i%30 == 0 and i != 0):\n","#                     df = pd.DataFrame({'key_id': ids, 'word': total_predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","            # df['predictions'] = df['predictions'].map(prediction_to_words)\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(f'{out_small}_{i//50}.csv', index=False)\n","            total_predictions = []\n","        else:\n","            total_predictions.append(pd.DataFrame({'key_id': ids, 'word': predictions}))\n","    else: total_predictions.append(predictions)\n","if not out: return total_predictions\n","else:\n","    if total_predictions: \n","        total_predictions = pd.concat(total_predictions)\n","        total_predictions.to_csv(f'{out_small}_{i}.csv', index=False)\n","    total_predictions = []\n","    for j in range(i+1):\n","        total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","    total_predictions = pd.concat(total_predictions)\n","    total_predictions.to_csv(out, index=False)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:48:17.654886Z","iopub.status.busy":"2023-09-16T04:48:17.654001Z","iopub.status.idle":"2023-09-16T04:48:18.222179Z","shell.execute_reply":"2023-09-16T04:48:18.220940Z","shell.execute_reply.started":"2023-09-16T04:48:17.654851Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[              key_id                             word\n"," 0   5480749689995264      baseball_bat leg paper_clip\n"," 1   4551472060563456       diving_board whale mermaid\n"," 2   6017422093975552     triangle bowtie diving_board\n"," 3   6306964361445376              bowtie frog octagon\n"," 4   5551625710075904  diving_board anvil fire_hydrant\n"," ..               ...                              ...\n"," 90  4893585587568640        bowtie piano diving_board\n"," 91  6040905817522176               nail lollipop fork\n"," 92  4607485933518848           hand baseball_bat pear\n"," 93  5693868500582400           hexagon octagon bowtie\n"," 94  5205682250842112        diving_board bridge piano\n"," \n"," [95 rows x 2 columns]]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["pred_list = []\n","total_predictions = []\n","# for last in iter(test_loader):\n","#     pass\n","out = 'submission.csv'\n","out_small = 'sub'\n","data, ids = last\n","# print(data.shape)\n","i = 29*50\n","for j in range(5):\n","    with torch.no_grad():\n","        model = res_models[j]\n","        if j < 2:pred_list.append(model.forward(data, device).reshape(1, 95, 101))\n","        else:pred_list.append(model.forward(data, device=device2).reshape(1, 95, 101))\n","pred_list = torch.concatenate(pred_list)\n","pred_list = pred_list.sum(0)\n","#     print(pred_list)\n","predictions = torch.topk(pred_list, 3, dim=1)[1]\n","predictions = (prediction_to_words(p) for p in predictions)\n","if out:\n","    if (i%30 == 0 and i != 0):\n","#                     df = pd.DataFrame({'key_id': ids, 'word': total_predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","        # df['predictions'] = df['predictions'].map(prediction_to_words)\n","        total_predictions = pd.concat(total_predictions)\n","        total_predictions.to_csv(f'{out_small}_{i//50}.csv', index=False)\n","        print(i//50)\n","        total_predictions = []\n","    else:\n","        total_predictions.append(pd.DataFrame({'key_id': ids, 'word': predictions}))\n","else: total_predictions.append(predictions)\n","total_predictions"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T04:49:40.261014Z","iopub.status.busy":"2023-09-16T04:49:40.260384Z","iopub.status.idle":"2023-09-16T04:49:40.797359Z","shell.execute_reply":"2023-09-16T04:49:40.796081Z","shell.execute_reply.started":"2023-09-16T04:49:40.260972Z"},"trusted":true},"outputs":[],"source":["if total_predictions: \n","    total_predictions = pd.concat(total_predictions)\n","    total_predictions.to_csv(f'{out_small}_{29}.csv', index=False)\n","total_predictions = []\n","for j in range(i+1):\n","    if j!=30:total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","    else: \n","#         total_predictions.append(pd.read_csv('sub_1448.csv'))\n","        break\n","total_predictions = pd.concat(total_predictions)\n","total_predictions.to_csv(out, index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
