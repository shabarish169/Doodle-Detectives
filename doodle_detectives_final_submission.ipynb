{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:22.725252Z","iopub.status.busy":"2023-09-15T11:14:22.724463Z","iopub.status.idle":"2023-09-15T11:14:22.732129Z","shell.execute_reply":"2023-09-15T11:14:22.731074Z","shell.execute_reply.started":"2023-09-15T11:14:22.725217Z"},"trusted":true},"outputs":[],"source":["import numpy as np, pandas as pd, matplotlib.pyplot as plt\n","import torch, torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from sklearn import preprocessing\n","from sklearn.model_selection import StratifiedKFold, KFold\n","import torchvision.models, torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","device = torch.device('cuda')\n","\n","\"\"\" This code ran fine on kaggle as a submitted notebook,\n","but there is a bug/limit on kaggle when you have more than 500 output files, \n","so submission.csv was not saved or submitted, but the weights were saved. \n","I made a dataset with the saved weights, but I don't have the exact code I used to rerun the last 2 cells.\n","Basically .load_state_dict(torch.load()) followed by res_model.predict() \n","(there is no number limit when running in kaggle without save and commit all) \"\"\"\n","# Our first submission used similar code with the BasicCNN class\n","# KFold was not fully used in this submission to save time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:22.738336Z","iopub.status.busy":"2023-09-15T11:14:22.737665Z","iopub.status.idle":"2023-09-15T11:14:22.748068Z","shell.execute_reply":"2023-09-15T11:14:22.746887Z","shell.execute_reply.started":"2023-09-15T11:14:22.738297Z"},"trusted":true},"outputs":[],"source":["# import gc\n","# gc.collect()\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:22.752690Z","iopub.status.busy":"2023-09-15T11:14:22.751916Z","iopub.status.idle":"2023-09-15T11:14:22.764570Z","shell.execute_reply":"2023-09-15T11:14:22.763377Z","shell.execute_reply.started":"2023-09-15T11:14:22.752657Z"},"trusted":true},"outputs":[],"source":["from PIL import Image, ImageDraw\n","import numpy as np\n","import json\n","\n","def vector_to_numpy(drawing, side=256):\n","    image = vector_to_image(drawing, side)\n","    image_array = np.array(image)\n","    return image_array\n","\n","def vector_to_image(drawing, side=256):\n","    drawing = json.loads(drawing)\n","    min_x, min_y, max_x, max_y = calculate_bounding_box(drawing)\n","\n","    # Calculate the offset to center the drawing within the canvas\n","    offset_x = (side - (max_x - min_x + 1)) // 2\n","    offset_y = (side - (max_y - min_y + 1)) // 2\n","\n","    image = Image.new('L', (side, side), color='white')  # Create a white canvas\n","    draw = ImageDraw.Draw(image)\n","\n","    for x, y in drawing:\n","        xy = [(x0 - min_x + offset_x, y0 - min_y + offset_y) for x0, y0 in zip(x, y)]\n","        draw.line(xy, fill='black', width=1)\n","\n","    return image\n","\n","def calculate_bounding_box(drawing):\n","    all_x = [x for x, _ in drawing]\n","    all_y = [y for _, y in drawing]\n","\n","    min_x = min(min(x) for x in all_x)\n","    min_y = min(min(y) for y in all_y)\n","    max_x = max(max(x) for x in all_x)\n","    max_y = max(max(y) for y in all_y)\n","\n","    return min_x, min_y, max_x, max_y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:22.766451Z","iopub.status.busy":"2023-09-15T11:14:22.766042Z","iopub.status.idle":"2023-09-15T11:14:25.054627Z","shell.execute_reply":"2023-09-15T11:14:25.053560Z","shell.execute_reply.started":"2023-09-15T11:14:22.766419Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/train.csv') #, dtype={'drawing': np.array})\n","class_list = df['word'].unique()\n","classes = {word: index for index, word in enumerate(class_list)}\n","def prediction_to_words(prediction):\n","    return ' '.join((class_list[p] for p in prediction))\n","# df = df[df['recognized']==True].reset_index().drop('index', axis=1)\n","df = df[df['recognized']==True].sample(n=100_000).reset_index().drop('index', axis=1)\n","# df = df[df['word'].isin(class_list[:10])].reset_index().drop('index', axis=1)\n","# df['drawing'] = df['drawing'].map(vector_to_numpy)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:25.061994Z","iopub.status.busy":"2023-09-15T11:14:25.059700Z","iopub.status.idle":"2023-09-15T11:14:25.068341Z","shell.execute_reply":"2023-09-15T11:14:25.067159Z","shell.execute_reply.started":"2023-09-15T11:14:25.061964Z"},"trusted":true},"outputs":[],"source":["# print(classes, class_list, sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:25.071733Z","iopub.status.busy":"2023-09-15T11:14:25.071313Z","iopub.status.idle":"2023-09-15T11:14:27.176958Z","shell.execute_reply":"2023-09-15T11:14:27.175922Z","shell.execute_reply.started":"2023-09-15T11:14:25.071699Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","#      transforms.Resize((28, 28)),\n","     transforms.Lambda(lambda x: x.repeat(1,3, 1, 1)),\n","    #  transforms.Lambda(lambda x: print(x.shape)),\n","    #  transforms.Grayscale(num_output_channels=3),\n","    #  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","     ])\n","vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n","# inception_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n","inception_v3 = torchvision.models.inception_v3(weights=torchvision.models.Inception_V3_Weights.DEFAULT)\n","inception_v3.aux_logits = False\n","resnet_18 = torchvision.models.resnet18()#weights=torchvision.models.ResNet18_Weights.DEFAULT)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.178789Z","iopub.status.busy":"2023-09-15T11:14:27.178439Z","iopub.status.idle":"2023-09-15T11:14:27.199268Z","shell.execute_reply":"2023-09-15T11:14:27.198298Z","shell.execute_reply.started":"2023-09-15T11:14:27.178755Z"},"trusted":true},"outputs":[],"source":["class BasicCNN(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = nn.Sequential(\n","            nn.MaxPool2d(8),\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n","            # 1 input image. If we had an RGB image, it would be Conv2d(3, 32, 3, padding=1)\n","            # 32 output images, i.e, 32 kernels and 32 output images are produced\n","            nn.ReLU(),\n","            # The activation function\n","            nn.MaxPool2d(2),\n","            # Pooling with 2 x 2 blocks\n","            nn.Conv2d(32, 64, 3, padding=1),\n","            # Now we have those 32 images and we make 64 from them\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","            # Pooling again\n","        )\n","        self.fully_connected = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear((28*28*64)//16, 512),\n","            # The image shape was initially 28 x 28, by pooling we've made it 7 x 7, so we divide by 16\n","            # We multiply by 64 because the model has learnt 64 features.\n","            nn.Linear(512, 101),\n","#             nn.Linear(128, 101)\n","            # We have 10 output neurons (1 for each class)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transforms.Compose([transforms.Resize((28, 28))])(inputs)\n","        x = self.convolutions(inputs.to(device))\n","        # Functions in convolution layers are run\n","        x = self.fully_connected(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='SUBMISSION.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)\n","\n","# le = preprocessing.LabelEncoder()\n","class MyDataset():\n","    def __init__(self, data, targets=None, ids=None, train=True, size=224):\n","        self.data = data\n","        self.train = train\n","        self.size = size\n","        if train: \n","            self.targets = targets.map(lambda target: classes[target])\n","        if ids is not None: self.ids = ids\n","    def __len__(self):\n","        return len(self.data)\n","    def __getitem__(self, i):\n","        img = 1 - vector_to_numpy(self.data.loc[i], side=self.size)//250\n","        if self.train: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.targets[i], dtype=torch.int64)\n","        if self.ids is not None: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.ids[i], dtype=torch.int64)\n","        return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.201546Z","iopub.status.busy":"2023-09-15T11:14:27.200875Z","iopub.status.idle":"2023-09-15T11:14:27.219871Z","shell.execute_reply":"2023-09-15T11:14:27.218968Z","shell.execute_reply.started":"2023-09-15T11:14:27.201511Z"},"trusted":true},"outputs":[],"source":["i = 0\n","vgg_length = len(list(vgg16.features.parameters()))\n","for param in vgg16.features.parameters():\n","    if i < vgg_length - 4: param.requires_grad = False\n","i = 0\n","inc_length = len(list(inception_v3.parameters()))\n","for param in inception_v3.parameters():\n","    if i < inc_length - 3: param.requires_grad = False\n","for param in resnet_18.parameters():\n","    param.requires_grad = True\n","resnet_18.fc = nn.Linear(512, 101)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.221793Z","iopub.status.busy":"2023-09-15T11:14:27.221465Z","iopub.status.idle":"2023-09-15T11:14:27.235549Z","shell.execute_reply":"2023-09-15T11:14:27.234544Z","shell.execute_reply.started":"2023-09-15T11:14:27.221762Z"},"trusted":true},"outputs":[],"source":["class VGG16(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = vgg16\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","        inputs = transform(inputs).to(device=device)\n","        x = self.convolutions(inputs)\n","        x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.237529Z","iopub.status.busy":"2023-09-15T11:14:27.237176Z","iopub.status.idle":"2023-09-15T11:14:27.253436Z","shell.execute_reply":"2023-09-15T11:14:27.252492Z","shell.execute_reply.started":"2023-09-15T11:14:27.237498Z"},"trusted":true},"outputs":[],"source":["class Inception(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = inception_v3\n","\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","#             nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transform(inputs).to(device=device)\n","        inputs = transforms.Compose([transforms.Resize((299, 299)),\n","                                    transforms.Lambda(lambda x: x.repeat(1,3, 1, 1))])(inputs)\n","        x = self.convolutions(inputs.to(device))\n","        x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.258992Z","iopub.status.busy":"2023-09-15T11:14:27.258247Z","iopub.status.idle":"2023-09-15T11:14:27.272046Z","shell.execute_reply":"2023-09-15T11:14:27.270852Z","shell.execute_reply.started":"2023-09-15T11:14:27.258967Z"},"trusted":true},"outputs":[],"source":["class ResNet18(nn.Module):\n","    def __init__(self):\n","        nn.Module.__init__(self)\n","        self.convolutions = resnet_18\n","\n","        self.extra = nn.Sequential(\n","            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n","            nn.Softmax(dim=1)\n","        )\n","    def forward(self, inputs):\n","        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n","        # print(type(inputs))\n","#         inputs = transform(inputs).to(device=device)\n","        inputs = transforms.Compose([\n","#                                     transforms.Resize((299, 299)),\n","                                    transforms.Lambda(lambda x: x.repeat(1,3, 1, 1))])(inputs)\n","        x = self.convolutions(inputs.to(device))\n","#         x = self.extra(x)\n","        # Functions in fully connected layer are run\n","        return x\n","    def predict(self, test_loader, out='s.csv', out_small='sub', i_0=0): # out = None may not be implemented\n","        \"\"\"\n","        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n","        \"\"\"\n","        self.eval()\n","        if not out: total_predictions=[]\n","        for i, (data, ids) in enumerate(iter(test_loader)):\n","            if i_0 > i: continue\n","            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n","            predictions = (prediction_to_words(p) for p in predictions)\n","            if out:\n","                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n","                # df['predictions'] = df['predictions'].map(prediction_to_words)\n","                df.to_csv(f'{out_small}_{i}.csv', index=False)\n","            else: total_predictions.append(predictions)\n","        if not out: return total_predictions\n","        else:\n","            total_predictions = []\n","            for j in range(i+1):\n","                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n","            total_predictions = pd.concat(total_predictions)\n","            total_predictions.to_csv(out, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.275401Z","iopub.status.busy":"2023-09-15T11:14:27.275028Z","iopub.status.idle":"2023-09-15T11:14:27.289766Z","shell.execute_reply":"2023-09-15T11:14:27.288800Z","shell.execute_reply.started":"2023-09-15T11:14:27.275321Z"},"trusted":true},"outputs":[],"source":["# Defining one epoch of training\n","def train(model, train_loader, optimizer, loss, ps=50):\n","    # We train the appropriate model with the input data and the appropriate optimizer\n","    # ps is how often we print the accuracy\n","    train_iter = iter(train_loader)\n","    model.train()\n","    # Puts model in train mode\n","    for i, (data, targets) in enumerate(train_iter):\n","        # i is iteration, data = 1 mini batch of images, targets = 1 mini batch target values\n","        # This repeats for all mini batches \n","        # print(targets)\n","        targets = targets.to(device)\n","        outputs = model.forward(data) # Forward pass\n","        loss_val = loss(outputs, targets) # Loss computation\n","        # print(targets)\n","        optimizer.zero_grad()  # Ensures gradients stored in optimizer are reset before each backward pass\n","        loss_val.backward() # Backward pass\n","        optimizer.step() # Backward pass\n","\n","        if ps and i % ps == 0:\n","            model.eval()\n","            # Puts model in evaluation mode, so we \n","            with torch.no_grad():\n","                print(f\"Batch {i}: Loss is {loss_val}\", end='; ')\n","                predicted = outputs.max(1)[1]\n","                correct = (predicted == targets).sum().item()\n","                accuracy = correct/len(targets)\n","                print(f\"Train accuracy is {accuracy*100:.3f}%\")\n","def accuracy(model, test):\n","    # Evaluate a model given a test loader\n","    model.eval()\n","    with torch.no_grad():\n","        count = 0\n","        correct = 0\n","        for data, targets in iter(test):\n","            targets = targets.to(device)\n","            outputs = model.forward(data)\n","#             predicted = outputs.max(1)[1] # Maximum output is predicted class\n","            predictions = torch.topk(outputs, 3, dim=1)[1]\n","            pred_1 = predictions[:, 0]\n","            pred_2 = predictions[:, 1]\n","            pred_3 = predictions[:, 2]\n","            count += len(targets) # Total length of datasetS\n","            correct += (pred_1 == targets).sum().item() + (pred_2 == targets).sum().item() + (pred_3 == targets).sum().item()\n","            # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n","        # print((predicted == targets).sum().item())\n","        accuracy = correct/count\n","        return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.292645Z","iopub.status.busy":"2023-09-15T11:14:27.292216Z","iopub.status.idle":"2023-09-15T11:14:27.320772Z","shell.execute_reply":"2023-09-15T11:14:27.319887Z","shell.execute_reply.started":"2023-09-15T11:14:27.292614Z"},"trusted":true},"outputs":[],"source":["basic_cnn = BasicCNN().to(device)\n","cnn_optimizer = torch.optim.Adam(basic_cnn.parameters(), lr=2e-3)\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.322562Z","iopub.status.busy":"2023-09-15T11:14:27.322221Z","iopub.status.idle":"2023-09-15T11:14:27.411154Z","shell.execute_reply":"2023-09-15T11:14:27.410202Z","shell.execute_reply.started":"2023-09-15T11:14:27.322532Z"},"trusted":true},"outputs":[],"source":["for train_index, val_index in KFold(n_splits=5).split(df['drawing'], df['word']):\n","    df_train = df.loc[train_index].reset_index().drop('index', axis=1)\n","    df_val = df.loc[val_index].reset_index().drop('index', axis=1)\n","    break\n","train_dataset = MyDataset(df_train['drawing'], df_train['word'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.413316Z","iopub.status.busy":"2023-09-15T11:14:27.412907Z","iopub.status.idle":"2023-09-15T11:14:27.419183Z","shell.execute_reply":"2023-09-15T11:14:27.418058Z","shell.execute_reply.started":"2023-09-15T11:14:27.413275Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.421259Z","iopub.status.busy":"2023-09-15T11:14:27.420908Z","iopub.status.idle":"2023-09-15T11:14:27.431368Z","shell.execute_reply":"2023-09-15T11:14:27.430401Z","shell.execute_reply.started":"2023-09-15T11:14:27.421225Z"},"trusted":true},"outputs":[],"source":["# print(next(iter(train_loader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.432771Z","iopub.status.busy":"2023-09-15T11:14:27.432485Z","iopub.status.idle":"2023-09-15T11:14:27.454009Z","shell.execute_reply":"2023-09-15T11:14:27.452920Z","shell.execute_reply.started":"2023-09-15T11:14:27.432742Z"},"trusted":true},"outputs":[],"source":["# train_loader.targets.loc[0]\n","df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.455832Z","iopub.status.busy":"2023-09-15T11:14:27.455491Z","iopub.status.idle":"2023-09-15T11:14:27.467367Z","shell.execute_reply":"2023-09-15T11:14:27.466127Z","shell.execute_reply.started":"2023-09-15T11:14:27.455794Z"},"trusted":true},"outputs":[],"source":["# train(basic_cnn, train_loader, cnn_optimizer, loss_fn)\n","# torch.save(basic_cnn.state_dict(), 'first_model.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.469907Z","iopub.status.busy":"2023-09-15T11:14:27.469216Z","iopub.status.idle":"2023-09-15T11:14:27.480796Z","shell.execute_reply":"2023-09-15T11:14:27.479743Z","shell.execute_reply.started":"2023-09-15T11:14:27.469874Z"},"trusted":true},"outputs":[],"source":["# vgg16_model = VGG16()\n","# vgg16_model.to(device)\n","# vgg16_optim = torch.optim.Adam(vgg16_model.parameters(), lr=2e-3)\n","# vgg_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.482634Z","iopub.status.busy":"2023-09-15T11:14:27.482241Z","iopub.status.idle":"2023-09-15T11:14:27.494734Z","shell.execute_reply":"2023-09-15T11:14:27.493721Z","shell.execute_reply.started":"2023-09-15T11:14:27.482600Z"},"trusted":true},"outputs":[],"source":["# inception_v3_model = Inception()\n","# inception_v3_model.to(device)\n","# inc_optim = torch.optim.Adam(inception_v3_model.parameters(), lr=2e-3)\n","# inc_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.497317Z","iopub.status.busy":"2023-09-15T11:14:27.496701Z","iopub.status.idle":"2023-09-15T11:14:27.508998Z","shell.execute_reply":"2023-09-15T11:14:27.507953Z","shell.execute_reply.started":"2023-09-15T11:14:27.497282Z"},"trusted":true},"outputs":[],"source":["# # train_dataset = MyDataset(df_train['drawing'], df_train['word'], size=299)\n","# # train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","# train(inception_v3_model, train_loader, inc_optim, inc_loss)\n","# torch.save(inception_v3_model.state_dict(), 'inception_v3_model.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.510948Z","iopub.status.busy":"2023-09-15T11:14:27.510391Z","iopub.status.idle":"2023-09-15T11:14:27.542938Z","shell.execute_reply":"2023-09-15T11:14:27.542089Z","shell.execute_reply.started":"2023-09-15T11:14:27.510912Z"},"trusted":true},"outputs":[],"source":["res_model = ResNet18()\n","res_model.to(device)\n","res_optim = torch.optim.Adam(res_model.parameters(), lr=2e-4)\n","res_loss = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-15T11:14:27.545537Z","iopub.status.busy":"2023-09-15T11:14:27.544983Z","iopub.status.idle":"2023-09-15T11:19:18.266831Z","shell.execute_reply":"2023-09-15T11:19:18.265302Z","shell.execute_reply.started":"2023-09-15T11:14:27.545505Z"},"trusted":true},"outputs":[],"source":["for j in range(2):\n","    for i in range(10):\n","        print(f'Epoch {j*10+i+1}/20\\n')\n","        train(res_model, train_loader, res_optim, res_loss)\n","        torch.save(res_model.state_dict(), 'ResNet18.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.267732Z","iopub.status.idle":"2023-09-15T11:19:18.268091Z","shell.execute_reply":"2023-09-15T11:19:18.267937Z","shell.execute_reply.started":"2023-09-15T11:19:18.267915Z"},"trusted":true},"outputs":[],"source":["val_set = MyDataset(df_val['drawing'], df_val['word'])\n","val_loader = DataLoader(val_set, batch_size=128)\n","# print(accuracy(basic_cnn, val_loader))/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.272981Z","iopub.status.idle":"2023-09-15T11:19:18.273783Z","shell.execute_reply":"2023-09-15T11:19:18.273557Z","shell.execute_reply.started":"2023-09-15T11:19:18.273534Z"},"trusted":true},"outputs":[],"source":["dft = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/test.csv')#, dtype={'drawing': np.array})\n","# dft['drawing'] = dft['drawing'].map(vector_to_numpy)\n","# dft = dft[dft['word'].isin(class_list[:10])].reset_index().drop('index', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.275204Z","iopub.status.idle":"2023-09-15T11:19:18.276030Z","shell.execute_reply":"2023-09-15T11:19:18.275798Z","shell.execute_reply.started":"2023-09-15T11:19:18.275774Z"},"trusted":true},"outputs":[],"source":["test_dataset = MyDataset(dft['drawing'], train=False, ids=dft['key_id'])\n","test_loader = DataLoader(test_dataset, batch_size=128, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.277442Z","iopub.status.idle":"2023-09-15T11:19:18.278204Z","shell.execute_reply":"2023-09-15T11:19:18.277976Z","shell.execute_reply.started":"2023-09-15T11:19:18.277952Z"},"trusted":true},"outputs":[],"source":["# print(accuracy(vgg16_model, val_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.279587Z","iopub.status.idle":"2023-09-15T11:19:18.280365Z","shell.execute_reply":"2023-09-15T11:19:18.280135Z","shell.execute_reply.started":"2023-09-15T11:19:18.280084Z"},"trusted":true},"outputs":[],"source":["# vgg16_model.predict(test_loader, i_0=406)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.281814Z","iopub.status.idle":"2023-09-15T11:19:18.282660Z","shell.execute_reply":"2023-09-15T11:19:18.282426Z","shell.execute_reply.started":"2023-09-15T11:19:18.282402Z"},"trusted":true},"outputs":[],"source":["# basic_cnn_2 = BasicCNN()\n","# basic_cnn_2.load_state_dict(torch.load('first_model.pt'))\n","# # basic_cnn_2.predict(test_loader)\n","# basic_cnn_3 = BasicCNN()\n","# # train(basic_cnn_3, train_loader, cnn_optimizer, loss_fn, ps=2)\n","\n","# with torch.no_grad():\n","#     print(basic_cnn_2.forward(next(iter(test_loader))[0]))\n","# # basic_cnn.eval()\n","# with torch.no_grad():\n","#     count = 0\n","#     correct = 0\n","#     for data, targets in iter(dataloader):\n","#         outputs = model.forward(data)\n","#         predicted = outputs.max(1)[1] # Maximum output is predicted class\n","#         count += len(targets) # Total length of datasetS\n","#         correct += (predicted == targets).sum().item()\n","#         # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n","#     print((predicted == targets).sum().item())\n","#     accuracy = correct/count\n","#     return accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.284042Z","iopub.status.idle":"2023-09-15T11:19:18.284933Z","shell.execute_reply":"2023-09-15T11:19:18.284689Z","shell.execute_reply.started":"2023-09-15T11:19:18.284664Z"},"trusted":true},"outputs":[],"source":["# print(accuracy(basic_cnn, val_loader))\n","# print(accuracy(inception_v3_model, val_loader))\n","print(accuracy(res_model, val_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:19:18.286421Z","iopub.status.idle":"2023-09-15T11:19:18.287217Z","shell.execute_reply":"2023-09-15T11:19:18.286977Z","shell.execute_reply.started":"2023-09-15T11:19:18.286952Z"},"trusted":true},"outputs":[],"source":["# basic_cnn.predict(test_loader)\n","# inception_v3_model.predict(test_loader)\n","# res_model.predict(test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["res_model.predict(test_loader, out='submission.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
