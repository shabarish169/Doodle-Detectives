{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport torchvision.models, torchvision.transforms as transforms\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\ndevice = torch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:36.068598Z","iopub.execute_input":"2023-09-11T15:09:36.069082Z","iopub.status.idle":"2023-09-11T15:09:36.076559Z","shell.execute_reply.started":"2023-09-11T15:09:36.069047Z","shell.execute_reply":"2023-09-11T15:09:36.075416Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from PIL import Image, ImageDraw\nimport numpy as np\nimport json\n\ndef vector_to_numpy(drawing, side=256):\n    image = vector_to_image(drawing, side)\n    image_array = np.array(image)\n    return image_array\n\ndef vector_to_image(drawing, side=256):\n    drawing = json.loads(drawing)\n    min_x, min_y, max_x, max_y = calculate_bounding_box(drawing)\n\n    # Calculate the offset to center the drawing within the canvas\n    offset_x = (side - (max_x - min_x + 1)) // 2\n    offset_y = (side - (max_y - min_y + 1)) // 2\n\n    image = Image.new('L', (side, side), color='white')  # Create a white canvas\n    draw = ImageDraw.Draw(image)\n\n    for x, y in drawing:\n        xy = [(x0 - min_x + offset_x, y0 - min_y + offset_y) for x0, y0 in zip(x, y)]\n        draw.line(xy, fill='black', width=1)\n\n    return image\n\ndef calculate_bounding_box(drawing):\n    all_x = [x for x, _ in drawing]\n    all_y = [y for _, y in drawing]\n\n    min_x = min(min(x) for x in all_x)\n    min_y = min(min(y) for y in all_y)\n    max_x = max(max(x) for x in all_x)\n    max_y = max(max(y) for y in all_y)\n\n    return min_x, min_y, max_x, max_y","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:36.079206Z","iopub.execute_input":"2023-09-11T15:09:36.079933Z","iopub.status.idle":"2023-09-11T15:09:36.096102Z","shell.execute_reply.started":"2023-09-11T15:09:36.079897Z","shell.execute_reply":"2023-09-11T15:09:36.094988Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/train.csv') #, dtype={'drawing': np.array})\nclass_list = df['word'].unique()\nclasses = {word: index for index, word in enumerate(class_list)}\ndef prediction_to_words(prediction):\n    return ' '.join((class_list[p] for p in prediction))\ndf = df.sample(n=10000).reset_index().drop('index', axis=1)\n# df['drawing'] = df['drawing'].map(vector_to_numpy)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:36.098468Z","iopub.execute_input":"2023-09-11T15:09:36.098933Z","iopub.status.idle":"2023-09-11T15:09:38.493963Z","shell.execute_reply.started":"2023-09-11T15:09:36.098897Z","shell.execute_reply":"2023-09-11T15:09:38.492908Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose(\n    [\n    #  transforms.Resize((224, 224)),\n     transforms.Lambda(lambda x: x.repeat(1,3, 1, 1)),\n    #  transforms.Lambda(lambda x: print(x.shape)),\n    #  transforms.Grayscale(num_output_channels=3),\n    #  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n     ])\nvgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:38.499628Z","iopub.execute_input":"2023-09-11T15:09:38.499939Z","iopub.status.idle":"2023-09-11T15:09:40.087214Z","shell.execute_reply.started":"2023-09-11T15:09:38.499914Z","shell.execute_reply":"2023-09-11T15:09:40.086134Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class BasicCNN(nn.Module):\n    def __init__(self):\n        nn.Module.__init__(self)\n        self.convolutions = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            # 1 input image. If we had an RGB image, it would be Conv2d(3, 32, 3, padding=1)\n            # 32 output images, i.e, 32 kernels and 32 output images are produced\n            nn.ReLU(),\n            # The activation function\n            nn.MaxPool2d(4),\n            # Pooling with 2 x 2 blocks\n            nn.Conv2d(32, 64, 3, padding=1),\n            # Now we have those 32 images and we make 64 from them\n            nn.ReLU(),\n            nn.MaxPool2d(4)\n            # Pooling again\n        )\n        self.fully_connected = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear((224*224*64)//256, 600),\n            # The image shape was initially 28 x 28, by pooling we've made it 7 x 7, so we divide by 16\n            # We multiply by 64 because the model has learnt 64 features.\n            nn.Linear(600, 128),\n            nn.Linear(128, 101)\n            # We have 10 output neurons (1 for each class)\n        )\n    def forward(self, inputs):\n        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n        # print(type(inputs))\n        x = self.convolutions(inputs)\n        # Functions in convolution layers are run\n        x = self.fully_connected(x)\n        # Functions in fully connected layer are run\n        return x\n    def predict(self, test_loader, out='SUBMISSION.csv', out_small='sub', i_0=0): # out = None may not be implemented\n        \"\"\"\n        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n        \"\"\"\n        self.eval()\n        if not out: total_predictions=[]\n        for i, (data, ids) in enumerate(iter(test_loader)):\n            if i_0 > i: continue\n            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n            predictions = (prediction_to_words(p) for p in predictions)\n            if out:\n                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n                # df['predictions'] = df['predictions'].map(prediction_to_words)\n                df.to_csv(f'{out_small}_{i}.csv', index=False)\n            else: total_predictions.append(predictions)\n        if not out: return total_predictions\n        else:\n            total_predictions = []\n            for j in range(i+1):\n                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n            total_predictions = pd.concat(total_predictions)\n            total_predictions.to_csv(out, index=False)\n\n# le = preprocessing.LabelEncoder()\nclass MyDataset():\n    def __init__(self, data, targets=None, ids=None, train=True):\n        self.data = data\n        self.train = train\n        if train: \n            self.targets = targets.map(lambda target: classes[target])\n        if ids is not None: self.ids = ids\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, i):\n        img = 1 - vector_to_numpy(self.data.loc[i], side=224)//255\n        if self.train: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.targets[i], dtype=torch.int64)\n        if self.ids is not None: return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape))), torch.tensor(self.ids[i], dtype=torch.int64)\n        return (torch.tensor(img, dtype=torch.float32).reshape((1, *img.shape)))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.090571Z","iopub.execute_input":"2023-09-11T15:09:40.090977Z","iopub.status.idle":"2023-09-11T15:09:40.109988Z","shell.execute_reply.started":"2023-09-11T15:09:40.090941Z","shell.execute_reply":"2023-09-11T15:09:40.108680Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"i = 0\nvgg_length = len(list(vgg16.features.parameters()))\n# for param in vgg16.features.parameters():\n#     if i < vgg_length - 2: param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.111463Z","iopub.execute_input":"2023-09-11T15:09:40.112055Z","iopub.status.idle":"2023-09-11T15:09:40.126170Z","shell.execute_reply.started":"2023-09-11T15:09:40.112021Z","shell.execute_reply":"2023-09-11T15:09:40.125209Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self):\n        nn.Module.__init__(self)\n        self.convolutions = vgg16\n        self.extra = nn.Sequential(\n            nn.Linear(1000, 101), # This was my 1st guess at the no of outputs in VGG16, I was so surprised when it worked\n            nn.Softmax(dim=1)\n        )\n    def forward(self, inputs):\n        # inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\n        # print(type(inputs))\n        inputs = transform(inputs).to(device=device)\n        x = self.convolutions(inputs)\n        x = self.extra(x)\n        # Functions in fully connected layer are run\n        return x\n    def predict(self, test_loader, out='SUBMISSION.csv', out_small='sub', i_0=0): # out = None may not be implemented\n        \"\"\"\n        Returns the predictions in a csv chosen by out, i_0 is in case you crash and have already done some stuff\n        \"\"\"\n        self.eval()\n        if not out: total_predictions=[]\n        for i, (data, ids) in enumerate(iter(test_loader)):\n            if i_0 > i: continue\n            predictions = torch.topk(self.forward(data), 3, dim=1)[1]\n            predictions = (prediction_to_words(p) for p in predictions)\n            if out:\n                df = pd.DataFrame({'key_id': ids, 'word': predictions})# dtype={'key_id': np.int64, 'word': np.array})\n                # df['predictions'] = df['predictions'].map(prediction_to_words)\n                df.to_csv(f'{out_small}_{i}.csv', index=False)\n            else: total_predictions.append(predictions)\n        if not out: return total_predictions\n        else:\n            total_predictions = []\n            for j in range(i+1):\n                total_predictions.append(pd.read_csv(f'{out_small}_{j}.csv'))\n            total_predictions = pd.concat(total_predictions)\n            total_predictions.to_csv(out, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.127464Z","iopub.execute_input":"2023-09-11T15:09:40.128008Z","iopub.status.idle":"2023-09-11T15:09:40.143659Z","shell.execute_reply.started":"2023-09-11T15:09:40.127963Z","shell.execute_reply":"2023-09-11T15:09:40.142592Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Defining one epoch of training\ndef train(model, train_loader, optimizer, loss, ps=50):\n    # We train the appropriate model with the input data and the appropriate optimizer\n    # ps is how often we print the accuracy\n    train_iter = iter(train_loader)\n    model.train()\n    # Puts model in train mode\n    for i, (data, targets) in enumerate(train_iter):\n        # i is iteration, data = 1 mini batch of images, targets = 1 mini batch target values\n        # This repeats for all mini batches \n        # print(targets)\n        targets = targets.to(device)\n        outputs = model.forward(data) # Forward pass\n        loss_val = loss(outputs, targets) # Loss computation\n        # print(targets)\n        optimizer.zero_grad()  # Ensures gradients stored in optimizer are reset before each backward pass\n        loss_val.backward() # Backward pass\n        optimizer.step() # Backward pass\n\n        if ps and i % ps == 0:\n            model.eval()\n            # Puts model in evaluation mode, so we \n            with torch.no_grad():\n                print(f\"Loss is {loss_val}\")\n                predicted = outputs.max(1)[1]\n                correct = (predicted == targets).sum().item()\n                accuracy = correct/len(targets)\n                print(f\"Train accuracy is {accuracy*100:.3f}%\")\ndef accuracy(model, test):\n    # Evaluate a model given a test loader\n    model.eval()\n    with torch.no_grad():\n        count = 0\n        correct = 0\n        for data, targets in iter(test):\n            targets = targets.to(device)\n            outputs = model.forward(data)\n            predicted = outputs.max(1)[1] # Maximum output is predicted class\n            count += len(targets) # Total length of datasetS\n            correct += (predicted == targets).sum().item()\n            # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n        # print((predicted == targets).sum().item())\n        accuracy = correct/count\n        return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.145409Z","iopub.execute_input":"2023-09-11T15:09:40.145734Z","iopub.status.idle":"2023-09-11T15:09:40.158279Z","shell.execute_reply.started":"2023-09-11T15:09:40.145703Z","shell.execute_reply":"2023-09-11T15:09:40.157167Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"basic_cnn = BasicCNN()\ncnn_optimizer = torch.optim.Adam(basic_cnn.parameters(), lr=2e-3)\nloss_fn = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.159650Z","iopub.execute_input":"2023-09-11T15:09:40.159990Z","iopub.status.idle":"2023-09-11T15:09:40.242145Z","shell.execute_reply.started":"2023-09-11T15:09:40.159950Z","shell.execute_reply":"2023-09-11T15:09:40.241105Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"for train_index, val_index in KFold(n_splits=5).split(df['drawing'], df['word']):\n    df_train = df.loc[train_index].reset_index().drop('index', axis=1)\n    df_val = df.loc[val_index].reset_index().drop('index', axis=1)\n    break\ntrain_dataset = MyDataset(df_train['drawing'], df_train['word'])","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.243723Z","iopub.execute_input":"2023-09-11T15:09:40.244053Z","iopub.status.idle":"2023-09-11T15:09:40.265756Z","shell.execute_reply.started":"2023-09-11T15:09:40.244022Z","shell.execute_reply":"2023-09-11T15:09:40.264892Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.270942Z","iopub.execute_input":"2023-09-11T15:09:40.271261Z","iopub.status.idle":"2023-09-11T15:09:40.276189Z","shell.execute_reply.started":"2023-09-11T15:09:40.271203Z","shell.execute_reply":"2023-09-11T15:09:40.275169Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# print(next(iter(train_loader)))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.277982Z","iopub.execute_input":"2023-09-11T15:09:40.278485Z","iopub.status.idle":"2023-09-11T15:09:40.286691Z","shell.execute_reply.started":"2023-09-11T15:09:40.278455Z","shell.execute_reply":"2023-09-11T15:09:40.285240Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# train_loader.targets.loc[0]\ndf_train","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.288491Z","iopub.execute_input":"2023-09-11T15:09:40.288872Z","iopub.status.idle":"2023-09-11T15:09:40.307416Z","shell.execute_reply.started":"2023-09-11T15:09:40.288837Z","shell.execute_reply":"2023-09-11T15:09:40.306358Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                                                drawing            key_id  \\\n0                       [[[255, 137, 0], [0, 82, 214]]]  5940930873917440   \n1     [[[24, 70, 75, 74, 66, 26, 163, 255, 228, 211,...  6284814711783424   \n2     [[[3, 3, 37, 110, 146, 120, 53, 31, 0], [53, 2...  5973976620728320   \n3     [[[80, 72, 51, 36, 19, 6, 1, 0, 9, 16, 34, 82,...  6722227842056192   \n4     [[[1, 27, 57, 99, 136, 196, 175, 149, 116, 98,...  5491753161326592   \n...                                                 ...               ...   \n7995  [[[69, 71, 80], [129, 176, 255]], [[82, 77, 76...  5642310320652288   \n7996  [[[13, 0], [99, 155]], [[83, 66], [117, 184]],...  6698949417107456   \n7997  [[[74, 55, 49, 50, 59, 65, 74, 102, 121, 159, ...  6553356954763264   \n7998  [[[15, 8, 1], [22, 43, 100]], [[7, 24, 15], [6...  6261791757697024   \n7999  [[[48, 31, 23, 6, 0, 0, 5, 27, 62, 93, 120, 12...  4717168056860672   \n\n           word  recognized  \n0     lightning        True  \n1        pillow       False  \n2     lightning       False  \n3         apple        True  \n4        pillow       False  \n...         ...         ...  \n7995      sword       False  \n7996        bed       False  \n7997   elephant       False  \n7998     helmet       False  \n7999     potato        True  \n\n[8000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>drawing</th>\n      <th>key_id</th>\n      <th>word</th>\n      <th>recognized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[[255, 137, 0], [0, 82, 214]]]</td>\n      <td>5940930873917440</td>\n      <td>lightning</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[[24, 70, 75, 74, 66, 26, 163, 255, 228, 211,...</td>\n      <td>6284814711783424</td>\n      <td>pillow</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[[3, 3, 37, 110, 146, 120, 53, 31, 0], [53, 2...</td>\n      <td>5973976620728320</td>\n      <td>lightning</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[[80, 72, 51, 36, 19, 6, 1, 0, 9, 16, 34, 82,...</td>\n      <td>6722227842056192</td>\n      <td>apple</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[[1, 27, 57, 99, 136, 196, 175, 149, 116, 98,...</td>\n      <td>5491753161326592</td>\n      <td>pillow</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>[[[69, 71, 80], [129, 176, 255]], [[82, 77, 76...</td>\n      <td>5642310320652288</td>\n      <td>sword</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>[[[13, 0], [99, 155]], [[83, 66], [117, 184]],...</td>\n      <td>6698949417107456</td>\n      <td>bed</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>[[[74, 55, 49, 50, 59, 65, 74, 102, 121, 159, ...</td>\n      <td>6553356954763264</td>\n      <td>elephant</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>[[[15, 8, 1], [22, 43, 100]], [[7, 24, 15], [6...</td>\n      <td>6261791757697024</td>\n      <td>helmet</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7999</th>\n      <td>[[[48, 31, 23, 6, 0, 0, 5, 27, 62, 93, 120, 12...</td>\n      <td>4717168056860672</td>\n      <td>potato</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>8000 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train(basic_cnn, train_loader, cnn_optimizer, loss_fn)\n# torch.save(basic_cnn.state_dict(), 'first_model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.308954Z","iopub.execute_input":"2023-09-11T15:09:40.309307Z","iopub.status.idle":"2023-09-11T15:09:40.317347Z","shell.execute_reply.started":"2023-09-11T15:09:40.309275Z","shell.execute_reply":"2023-09-11T15:09:40.316191Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"vgg16_model = VGG16()\nvgg16_model.to(device)\nvgg16_optim = torch.optim.Adam(vgg16_model.parameters(), lr=2e-3)\nvgg_loss = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.318884Z","iopub.execute_input":"2023-09-11T15:09:40.319584Z","iopub.status.idle":"2023-09-11T15:09:40.487517Z","shell.execute_reply.started":"2023-09-11T15:09:40.319495Z","shell.execute_reply":"2023-09-11T15:09:40.486513Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"train(vgg16_model, train_loader, vgg16_optim, vgg_loss)\ntorch.save(basic_cnn.state_dict(), 'vgg16_model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:09:40.489075Z","iopub.execute_input":"2023-09-11T15:09:40.489513Z","iopub.status.idle":"2023-09-11T15:11:56.346689Z","shell.execute_reply.started":"2023-09-11T15:09:40.489477Z","shell.execute_reply":"2023-09-11T15:11:56.345582Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Loss is 4.616150856018066\nTrain accuracy is 0.781%\nLoss is 4.624176502227783\nTrain accuracy is 0.781%\n","output_type":"stream"}]},{"cell_type":"code","source":"val_set = MyDataset(df_val['drawing'], df_val['word'])\nval_loader = DataLoader(val_set, batch_size=128)\n# print(accuracy(basic_cnn, val_loader))/","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:11:56.348327Z","iopub.execute_input":"2023-09-11T15:11:56.348756Z","iopub.status.idle":"2023-09-11T15:11:56.358483Z","shell.execute_reply.started":"2023-09-11T15:11:56.348720Z","shell.execute_reply":"2023-09-11T15:11:56.357278Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"dft = pd.read_csv('/kaggle/input/doodle-detectives-aiclubiitm/test.csv')#, dtype={'drawing': np.array})\n# dft['drawing'] = dft['drawing'].map(vector_to_numpy)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:11:56.360467Z","iopub.execute_input":"2023-09-11T15:11:56.361382Z","iopub.status.idle":"2023-09-11T15:11:57.257944Z","shell.execute_reply.started":"2023-09-11T15:11:56.361329Z","shell.execute_reply":"2023-09-11T15:11:57.256908Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"test_dataset = MyDataset(dft['drawing'], train=False, ids=dft['key_id'])\ntest_loader = DataLoader(test_dataset, batch_size=128, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:11:57.259636Z","iopub.execute_input":"2023-09-11T15:11:57.260032Z","iopub.status.idle":"2023-09-11T15:11:57.266700Z","shell.execute_reply.started":"2023-09-11T15:11:57.259998Z","shell.execute_reply":"2023-09-11T15:11:57.265382Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"print(accuracy(vgg16_model, val_loader))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:11:57.268773Z","iopub.execute_input":"2023-09-11T15:11:57.269187Z","iopub.status.idle":"2023-09-11T15:12:08.521787Z","shell.execute_reply.started":"2023-09-11T15:11:57.269152Z","shell.execute_reply":"2023-09-11T15:12:08.520600Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"0.0155\n","output_type":"stream"}]},{"cell_type":"code","source":"vgg16_model.predict(test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:12:08.523535Z","iopub.execute_input":"2023-09-11T15:12:08.523924Z","iopub.status.idle":"2023-09-11T15:12:12.083499Z","shell.execute_reply.started":"2023-09-11T15:12:08.523888Z","shell.execute_reply":"2023-09-11T15:12:12.081352Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvgg16_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[52], line 25\u001b[0m, in \u001b[0;36mVGG16.predict\u001b[0;34m(self, test_loader, out, out_small, i_0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, ids) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader)):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i_0 \u001b[38;5;241m>\u001b[39m i: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     26\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m (prediction_to_words(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out:\n","Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mVGG16.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# inputs = inputs.reshape([inputs.shape[1], inputs.shape[0], 256, 256])\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(type(inputs))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m transform(inputs)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolutions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra(x)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Functions in fully connected layer are run\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"basic_cnn_2 = BasicCNN()\nbasic_cnn_2.load_state_dict(torch.load('first_model.pt'))\nbasic_cnn_2.predict(test_loader)\nbasic_cnn_3 = BasicCNN()\n# train(basic_cnn_3, train_loader, cnn_optimizer, loss_fn, ps=2)\n\n# with torch.no_grad():\n#     print(basic_cnn_2.forward(next(iter(test_loader))[0]))\n# # basic_cnn.eval()\n# with torch.no_grad():\n#     count = 0\n#     correct = 0\n#     for data, targets in iter(dataloader):\n#         outputs = model.forward(data)\n#         predicted = outputs.max(1)[1] # Maximum output is predicted class\n#         count += len(targets) # Total length of datasetS\n#         correct += (predicted == targets).sum().item()\n#         # This gives a tensor of True and False values and adds no. of True values to correct each iteration\n#     print((predicted == targets).sum().item())\n#     accuracy = correct/count\n#     return accuracy\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:12:12.084732Z","iopub.status.idle":"2023-09-11T15:12:12.085647Z","shell.execute_reply.started":"2023-09-11T15:12:12.085398Z","shell.execute_reply":"2023-09-11T15:12:12.085423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic_cnn_3.predict(test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T15:12:12.087523Z","iopub.status.idle":"2023-09-11T15:12:12.088064Z","shell.execute_reply.started":"2023-09-11T15:12:12.087780Z","shell.execute_reply":"2023-09-11T15:12:12.087806Z"},"trusted":true},"execution_count":null,"outputs":[]}]}